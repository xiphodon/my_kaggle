{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 获取数据\n",
    "这个项目的数据是从New York State Independent Service Authority (NYISO)获得的。他们把所有的历史数据都以CSV格式存储在FTP服务器上。 \n",
    "\n",
    "想了解更多的信息可以戳下面的页面: http://www.nyiso.com/public/markets_operations/market_data/load_data/index.jsp\n",
    "\n",
    "数据也可以从[这个页面](http://mis.nyiso.com/public/P-58Blist.htm)直接获得: `http://mis.nyiso.com/public/P-58Blist.htm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整个纽约州的电能由12个“区域”生产和提供，这12个区有自己独立的能源市场。下面这张图可以给你一个直观的印象，大概的一个分布状况。我们这里采集到的数据，也会按照这12个区域做分割，其中区域的名称会写在\"name\"字段里。\n",
    "\n",
    "<img src='https://business.directenergy.com/~/media/deb/images/callouts/map.ashx?la=en&h=500'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import urllib\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 直接下载数据\n",
    "我们直接找到数据源下载纽约州2001-2015年的相关数据。\n",
    "\n",
    "直接拉下来的数据会以zip形式存储在`../data/nyiso`文件夹下。 \n",
    "\n",
    "然后咱们解压缩打包文件到 `../data/nyiso/all/raw_data` 文件夹。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "download and unzipping...\n"
     ]
    }
   ],
   "source": [
    "print \"download and unzipping...\"\n",
    "dates = pd.date_range(pd.to_datetime('2001-01-01'), \\\n",
    "                       pd.to_datetime('2015-12-31'), freq='M')\n",
    "\n",
    "for date in dates:\n",
    "    url = 'http://mis.nyiso.com/public/csv/pal/{0}{1}01pal_csv.zip'.format(date.year, str(date.month).zfill(2))\n",
    "    urllib.urlretrieve(url, \"../data/nyiso/{0}\".format(url.split('/')[-1]))\n",
    "\n",
    "def unzip(source_filename, dest_dir):\n",
    "    with zipfile.ZipFile(source_filename) as zf:\n",
    "        try:\n",
    "            zf.extractall(dest_dir)\n",
    "        except:\n",
    "            print source_filename\n",
    "            return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/nyiso/20010101pal_csv.zip\n",
      "../data/nyiso/20010201pal_csv.zip\n",
      "../data/nyiso/20010301pal_csv.zip\n",
      "../data/nyiso/20010401pal_csv.zip\n",
      "../data/nyiso/20010501pal_csv.zipextract done!\n",
      "../data/nyiso/20010601pal_csv.zipextract done!\n",
      "../data/nyiso/20010701pal_csv.zipextract done!\n",
      "../data/nyiso/20010801pal_csv.zipextract done!\n",
      "../data/nyiso/20010901pal_csv.zipextract done!\n",
      "../data/nyiso/20011001pal_csv.zipextract done!\n",
      "../data/nyiso/20011101pal_csv.zipextract done!\n",
      "../data/nyiso/20011201pal_csv.zipextract done!\n",
      "../data/nyiso/20020101pal_csv.zipextract done!\n",
      "../data/nyiso/20020201pal_csv.zipextract done!\n",
      "../data/nyiso/20020301pal_csv.zipextract done!\n",
      "../data/nyiso/20020401pal_csv.zipextract done!\n",
      "../data/nyiso/20020501pal_csv.zipextract done!\n",
      "../data/nyiso/20020601pal_csv.zipextract done!\n",
      "../data/nyiso/20020701pal_csv.zipextract done!\n",
      "../data/nyiso/20020801pal_csv.zipextract done!\n",
      "../data/nyiso/20020901pal_csv.zipextract done!\n",
      "../data/nyiso/20021001pal_csv.zipextract done!\n",
      "../data/nyiso/20021101pal_csv.zipextract done!\n",
      "../data/nyiso/20021201pal_csv.zipextract done!\n",
      "../data/nyiso/20030101pal_csv.zipextract done!\n",
      "../data/nyiso/20030201pal_csv.zipextract done!\n",
      "../data/nyiso/20030301pal_csv.zipextract done!\n",
      "../data/nyiso/20030401pal_csv.zipextract done!\n",
      "../data/nyiso/20030501pal_csv.zipextract done!\n",
      "../data/nyiso/20030601pal_csv.zipextract done!\n",
      "../data/nyiso/20030701pal_csv.zipextract done!\n",
      "../data/nyiso/20030801pal_csv.zipextract done!\n",
      "../data/nyiso/20030901pal_csv.zipextract done!\n",
      "../data/nyiso/20031001pal_csv.zipextract done!\n",
      "../data/nyiso/20031101pal_csv.zipextract done!\n",
      "../data/nyiso/20031201pal_csv.zipextract done!\n",
      "../data/nyiso/20040101pal_csv.zipextract done!\n",
      "../data/nyiso/20040201pal_csv.zipextract done!\n",
      "../data/nyiso/20040301pal_csv.zipextract done!\n",
      "../data/nyiso/20040401pal_csv.zipextract done!\n",
      "../data/nyiso/20040501pal_csv.zipextract done!\n",
      "../data/nyiso/20040601pal_csv.zipextract done!\n",
      "../data/nyiso/20040701pal_csv.zipextract done!\n",
      "../data/nyiso/20040801pal_csv.zipextract done!\n",
      "../data/nyiso/20040901pal_csv.zipextract done!\n",
      "../data/nyiso/20041001pal_csv.zipextract done!\n",
      "../data/nyiso/20041101pal_csv.zipextract done!\n",
      "../data/nyiso/20041201pal_csv.zipextract done!\n",
      "../data/nyiso/20050101pal_csv.zipextract done!\n",
      "../data/nyiso/20050201pal_csv.zipextract done!\n",
      "../data/nyiso/20050301pal_csv.zipextract done!\n",
      "../data/nyiso/20050401pal_csv.zipextract done!\n",
      "../data/nyiso/20050501pal_csv.zipextract done!\n",
      "../data/nyiso/20050601pal_csv.zipextract done!\n",
      "../data/nyiso/20050701pal_csv.zipextract done!\n",
      "../data/nyiso/20050801pal_csv.zipextract done!\n",
      "../data/nyiso/20050901pal_csv.zipextract done!\n",
      "../data/nyiso/20051001pal_csv.zipextract done!\n",
      "../data/nyiso/20051101pal_csv.zipextract done!\n",
      "../data/nyiso/20051201pal_csv.zipextract done!\n",
      "../data/nyiso/20060101pal_csv.zipextract done!\n",
      "../data/nyiso/20060201pal_csv.zipextract done!\n",
      "../data/nyiso/20060301pal_csv.zipextract done!\n",
      "../data/nyiso/20060401pal_csv.zipextract done!\n",
      "../data/nyiso/20060501pal_csv.zipextract done!\n",
      "../data/nyiso/20060601pal_csv.zipextract done!\n",
      "../data/nyiso/20060701pal_csv.zipextract done!\n",
      "../data/nyiso/20060801pal_csv.zipextract done!\n",
      "../data/nyiso/20060901pal_csv.zipextract done!\n",
      "../data/nyiso/20061001pal_csv.zipextract done!\n",
      "../data/nyiso/20061101pal_csv.zipextract done!\n",
      "../data/nyiso/20061201pal_csv.zipextract done!\n",
      "../data/nyiso/20070101pal_csv.zipextract done!\n",
      "../data/nyiso/20070201pal_csv.zipextract done!\n",
      "../data/nyiso/20070301pal_csv.zipextract done!\n",
      "../data/nyiso/20070401pal_csv.zipextract done!\n",
      "../data/nyiso/20070501pal_csv.zipextract done!\n",
      "../data/nyiso/20070601pal_csv.zipextract done!\n",
      "../data/nyiso/20070701pal_csv.zipextract done!\n",
      "../data/nyiso/20070801pal_csv.zipextract done!\n",
      "../data/nyiso/20070901pal_csv.zipextract done!\n",
      "../data/nyiso/20071001pal_csv.zipextract done!\n",
      "../data/nyiso/20071101pal_csv.zipextract done!\n",
      "../data/nyiso/20071201pal_csv.zipextract done!\n",
      "../data/nyiso/20080101pal_csv.zipextract done!\n",
      "../data/nyiso/20080201pal_csv.zipextract done!\n",
      "../data/nyiso/20080301pal_csv.zipextract done!\n",
      "../data/nyiso/20080401pal_csv.zipextract done!\n",
      "../data/nyiso/20080501pal_csv.zipextract done!\n",
      "../data/nyiso/20080601pal_csv.zipextract done!\n",
      "../data/nyiso/20080701pal_csv.zipextract done!\n",
      "../data/nyiso/20080801pal_csv.zipextract done!\n",
      "../data/nyiso/20080901pal_csv.zipextract done!\n",
      "../data/nyiso/20081001pal_csv.zipextract done!\n",
      "../data/nyiso/20081101pal_csv.zipextract done!\n",
      "../data/nyiso/20081201pal_csv.zipextract done!\n",
      "../data/nyiso/20090101pal_csv.zipextract done!\n",
      "../data/nyiso/20090201pal_csv.zipextract done!\n",
      "../data/nyiso/20090301pal_csv.zipextract done!\n",
      "../data/nyiso/20090401pal_csv.zipextract done!\n",
      "../data/nyiso/20090501pal_csv.zipextract done!\n",
      "../data/nyiso/20090601pal_csv.zipextract done!\n",
      "../data/nyiso/20090701pal_csv.zipextract done!\n",
      "../data/nyiso/20090801pal_csv.zipextract done!\n",
      "../data/nyiso/20090901pal_csv.zipextract done!\n",
      "../data/nyiso/20091001pal_csv.zipextract done!\n",
      "../data/nyiso/20091101pal_csv.zipextract done!\n",
      "../data/nyiso/20091201pal_csv.zipextract done!\n",
      "../data/nyiso/20100101pal_csv.zipextract done!\n",
      "../data/nyiso/20100201pal_csv.zipextract done!\n",
      "../data/nyiso/20100301pal_csv.zipextract done!\n",
      "../data/nyiso/20100401pal_csv.zipextract done!\n",
      "../data/nyiso/20100501pal_csv.zipextract done!\n",
      "../data/nyiso/20100601pal_csv.zipextract done!\n",
      "../data/nyiso/20100701pal_csv.zipextract done!\n",
      "../data/nyiso/20100801pal_csv.zipextract done!\n",
      "../data/nyiso/20100901pal_csv.zipextract done!\n",
      "../data/nyiso/20101001pal_csv.zipextract done!\n",
      "../data/nyiso/20101101pal_csv.zipextract done!\n",
      "../data/nyiso/20101201pal_csv.zipextract done!\n",
      "../data/nyiso/20110101pal_csv.zipextract done!\n",
      "../data/nyiso/20110201pal_csv.zipextract done!\n",
      "../data/nyiso/20110301pal_csv.zipextract done!\n",
      "../data/nyiso/20110401pal_csv.zipextract done!\n",
      "../data/nyiso/20110501pal_csv.zipextract done!\n",
      "../data/nyiso/20110601pal_csv.zipextract done!\n",
      "../data/nyiso/20110701pal_csv.zipextract done!\n",
      "../data/nyiso/20110801pal_csv.zipextract done!\n",
      "../data/nyiso/20110901pal_csv.zipextract done!\n",
      "../data/nyiso/20111001pal_csv.zipextract done!\n",
      "../data/nyiso/20111101pal_csv.zipextract done!\n",
      "../data/nyiso/20111201pal_csv.zipextract done!\n",
      "../data/nyiso/20120101pal_csv.zipextract done!\n",
      "../data/nyiso/20120201pal_csv.zipextract done!\n",
      "../data/nyiso/20120301pal_csv.zipextract done!\n",
      "../data/nyiso/20120401pal_csv.zipextract done!\n",
      "../data/nyiso/20120501pal_csv.zipextract done!\n",
      "../data/nyiso/20120601pal_csv.zipextract done!\n",
      "../data/nyiso/20120701pal_csv.zipextract done!\n",
      "../data/nyiso/20120801pal_csv.zipextract done!\n",
      "../data/nyiso/20120901pal_csv.zipextract done!\n",
      "../data/nyiso/20121001pal_csv.zipextract done!\n",
      "../data/nyiso/20121101pal_csv.zipextract done!\n",
      "../data/nyiso/20121201pal_csv.zipextract done!\n",
      "../data/nyiso/20130101pal_csv.zipextract done!\n",
      "../data/nyiso/20130201pal_csv.zipextract done!\n",
      "../data/nyiso/20130301pal_csv.zipextract done!\n",
      "../data/nyiso/20130401pal_csv.zipextract done!\n",
      "../data/nyiso/20130501pal_csv.zipextract done!\n",
      "../data/nyiso/20130601pal_csv.zipextract done!\n",
      "../data/nyiso/20130701pal_csv.zipextract done!\n",
      "../data/nyiso/20130801pal_csv.zipextract done!\n",
      "../data/nyiso/20130901pal_csv.zipextract done!\n",
      "../data/nyiso/20131001pal_csv.zipextract done!\n",
      "../data/nyiso/20131101pal_csv.zipextract done!\n",
      "../data/nyiso/20131201pal_csv.zipextract done!\n",
      "../data/nyiso/20140101pal_csv.zipextract done!\n",
      "../data/nyiso/20140201pal_csv.zipextract done!\n",
      "../data/nyiso/20140301pal_csv.zipextract done!\n",
      "../data/nyiso/20140401pal_csv.zipextract done!\n",
      "../data/nyiso/20140501pal_csv.zipextract done!\n",
      "../data/nyiso/20140601pal_csv.zipextract done!\n",
      "../data/nyiso/20140701pal_csv.zipextract done!\n",
      "../data/nyiso/20140801pal_csv.zipextract done!\n",
      "../data/nyiso/20140901pal_csv.zipextract done!\n",
      "../data/nyiso/20141001pal_csv.zipextract done!\n",
      "../data/nyiso/20141101pal_csv.zipextract done!\n",
      "../data/nyiso/20141201pal_csv.zipextract done!\n",
      "../data/nyiso/20150101pal_csv.zipextract done!\n",
      "../data/nyiso/20150201pal_csv.zipextract done!\n",
      "../data/nyiso/20150301pal_csv.zipextract done!\n",
      "../data/nyiso/20150401pal_csv.zipextract done!\n",
      "../data/nyiso/20150501pal_csv.zipextract done!\n",
      "../data/nyiso/20150601pal_csv.zipextract done!\n",
      "../data/nyiso/20150701pal_csv.zipextract done!\n",
      "../data/nyiso/20150801pal_csv.zipextract done!\n",
      "../data/nyiso/20150901pal_csv.zipextract done!\n",
      "../data/nyiso/20151001pal_csv.zipextract done!\n",
      "../data/nyiso/20151101pal_csv.zipextract done!\n",
      "../data/nyiso/20151201pal_csv.zipextract done!\n"
     ]
    }
   ],
   "source": [
    "zips = []\n",
    "for file in os.listdir(\"../data/nyiso\"):\n",
    "    if file.endswith(\".zip\"):\n",
    "        zips.append(file)\n",
    "for z in zips:\n",
    "    try:\n",
    "        unzip('../data/nyiso/' + z, '../data/nyiso/all/raw_data')\n",
    "        print '../data/nyiso/' + z + \"extract done!\"\n",
    "    except:\n",
    "        print '../data/nyiso/' + z\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文件多了处理起来麻烦，咱们整合到一个合并的文件里吧: combined_nyiso.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "csvs = []\n",
    "for file in os.listdir(\"../data/nyiso/all/raw_data\"):\n",
    "    if file.endswith(\"pal.csv\"):\n",
    "        csvs.append(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fout=open(\"../data/nyiso/all/combined_iso.csv\",\"a\")\n",
    "\n",
    "# 第一个文件咱们保存头部column name信息:\n",
    "for line in open(\"../data/nyiso/all/raw_data/\"+csvs[0]):\n",
    "    fout.write(line)\n",
    "# 后面的部分可以跳过 headers:    \n",
    "for file in csvs[1:]:\n",
    "    f = open(\"../data/nyiso/all/raw_data/\"+file)\n",
    "    f.next() # 跳过 header\n",
    "    for line in f:\n",
    "         fout.write(line)\n",
    "    f.close() # 关闭文件\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 清洗和了解数据\n",
    "\n",
    "这里总共有14年的数据，我们把它们放到一个完整的pandas数据帧(dataframe)里。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/nyiso/all/combined_iso.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里所谓的数据清洗实际上是一个数据的选择过程，我们留下来我们感兴趣的4列: timestamp, region name, id, and load (说明一下，load是电力需求，单位是**兆瓦/Megawatts**). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = df.columns\n",
    "df.columns = [col.lower().replace(' ', '') for col in cols]\n",
    "df = df[['timestamp', 'name', 'ptid', 'load']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "重新把需要的数据写回csv文件中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('../data/nyiso/all/combined_iso.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CAPITL', 'CENTRL', 'DUNWOD', 'GENESE', 'HUD VL', 'MHK VL',\n",
       "       'MILLWD', 'N.Y.C._LONGIL', 'NORTH', 'WEST', 'LONGIL', 'N.Y.C.'], dtype=object)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.name.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立**天气站/weather stations**的映射关系\n",
    "\n",
    "我们的数据帧里区域的名称已经有了，我们要建立起它们和对应的城市还有天气站之间的映射关系（简单地理解就是需要关联几个数据表用）。我们直接把它们放在一个`python dict/字典`里，一会儿下一个notebook会用到这个映射关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "regions = list(df.name.unique())\n",
    "region_names = ['Capital', 'Central', 'Dunwoodie', 'Genese', 'Hudson Valley', 'Long Island', 'Mohawk Valley', 'Millwood', 'NYC', 'North', 'West']\n",
    "cities = ['Albany', 'Syracuse', 'Yonkers', 'Rochester', 'Poughkeepsie', 'NYC', 'Utica', 'Yonkers', 'NYC', 'Plattsburgh', 'Buffalo']\n",
    "weather_stations = ['kalb', 'ksyr', 'klga', 'kroc', 'kpou', 'kjfk', 'krme', 'klga', 'kjfk', 'kpbg', 'kbuf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CAPITL': ('kalb', 'Capital', 'Albany'),\n",
       " 'CENTRL': ('ksyr', 'Central', 'Syracuse'),\n",
       " 'DUNWOD': ('klga', 'Dunwoodie', 'Yonkers'),\n",
       " 'GENESE': ('kroc', 'Genese', 'Rochester'),\n",
       " 'HUD VL': ('kpou', 'Hudson Valley', 'Poughkeepsie'),\n",
       " 'LONGIL': ('kbuf', 'West', 'Buffalo'),\n",
       " 'MHK VL': ('kjfk', 'Long Island', 'NYC'),\n",
       " 'MILLWD': ('krme', 'Mohawk Valley', 'Utica'),\n",
       " 'N.Y.C._LONGIL': ('klga', 'Millwood', 'Yonkers'),\n",
       " 'NORTH': ('kjfk', 'NYC', 'NYC'),\n",
       " 'WEST': ('kpbg', 'North', 'Plattsburgh')}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weather_dict = dict(zip(regions, zip(weather_stations, region_names, cities)))\n",
    "weather_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import cPickle as pickle\n",
    "pickle.dump(weather_dict, open('weather_dict.pkl','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 把数据按区域切分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "我们按照12个区把数据做个切分，更细致的切分可以让天气数据更精确。同时在测试阶段，一个区一个统一的文件也会方便很多。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for region in weather_dict.keys():\n",
    "    subset = df[df.name == region].copy()\n",
    "    filename = weather_dict[region][1].lower().replace(' ', '') + '.csv'\n",
    "    subset.to_csv('../data/nyiso/all/' + filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>name</th>\n",
       "      <th>ptid</th>\n",
       "      <th>load</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-01-01 00:00:00</td>\n",
       "      <td>CAPITL</td>\n",
       "      <td>61757</td>\n",
       "      <td>1084.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-01-01 00:05:00</td>\n",
       "      <td>CAPITL</td>\n",
       "      <td>61757</td>\n",
       "      <td>1055.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-01-01 00:10:00</td>\n",
       "      <td>CAPITL</td>\n",
       "      <td>61757</td>\n",
       "      <td>1056.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-01-01 00:15:00</td>\n",
       "      <td>CAPITL</td>\n",
       "      <td>61757</td>\n",
       "      <td>1050.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-01-01 00:20:00</td>\n",
       "      <td>CAPITL</td>\n",
       "      <td>61757</td>\n",
       "      <td>1050.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            timestamp    name   ptid    load\n",
       "0 2012-01-01 00:00:00  CAPITL  61757  1084.4\n",
       "1 2012-01-01 00:05:00  CAPITL  61757  1055.3\n",
       "2 2012-01-01 00:10:00  CAPITL  61757  1056.6\n",
       "3 2012-01-01 00:15:00  CAPITL  61757  1050.8\n",
       "4 2012-01-01 00:20:00  CAPITL  61757  1050.8"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#其中的一个文件大概长这样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 输出2012年的数据用于测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output 2012 data for test...\n"
     ]
    }
   ],
   "source": [
    "print \"output 2012 data for test...\"\n",
    "capital = pd.read_csv(\"../data/nyiso/all/capital.csv\")\n",
    "#capital[capital.timestamp < pd.to_datetime('2013-01-01')].to_csv('load2012.csv', index=False)\n",
    "capital[capital.timestamp < '2013-01-01'].to_csv('load2012.csv', index=False)\n",
    "csvs = []\n",
    "for file in os.listdir(\"../data/wunderground/kalb\"):\n",
    "    if file.startswith(\"2012\"):\n",
    "        csvs.append(file)\n",
    "print csvs\n",
    "fout=open(\"weather2012.csv\",\"a\")\n",
    "\n",
    "# 写入整个文件:\n",
    "for line in open(\"../data/wunderground/kalb/\"+csvs[0]):\n",
    "    fout.write(line)\n",
    "# 跳过头部:    \n",
    "for file in csvs[1:]:\n",
    "    f = open(\"../data/wunderground/kalb/\"+file)\n",
    "    f.next()\n",
    "    for line in f:\n",
    "         fout.write(line)\n",
    "    f.close()\n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 下载NYISO的预测结果\n",
    "NYISO会公布一个\"day-ahead\"预测数据（每次提前一天他们都会预测下一条的用电需求状况）。 如果咱们能够比公司做的预测系统效果还要好，那妥妥的表示咱们的模型有比较好的效果。 我们先把它在2014-2016年的预测结果下载下来，以便最后比对。\n",
    "\n",
    "网址如下: http://www.nyiso.com/public/markets_operations/market_data/custom_report/index.jsp?report=load_forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyiso_forecast = pd.read_csv('../data/nyiso_dayahead_forecasts/forecast_2014_2016.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "211442"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nyiso_forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nyiso_forecast.columns = ['timestamp', 'zone', 'forecast', 'gmt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
